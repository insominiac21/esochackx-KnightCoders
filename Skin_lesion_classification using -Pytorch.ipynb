{
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kmader_skin_cancer_mnist_ham10000_path = kagglehub.dataset_download('kmader/skin-cancer-mnist-ham10000')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "w4GHIfcPs_ei"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bkPFUa9as_eo"
      },
      "cell_type": "markdown",
      "source": [
        " ### First, import all libraries that used in this project"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "iNv38pqFs_eq"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "# python libraties\n",
        "import os, cv2,itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "from torch import optim,nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import models,transforms\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# to make the results are reproducible\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "torch.cuda.manual_seed(10)\n",
        "\n",
        "print(os.listdir(\"../input\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9XhgJ_Js_eq"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1. Data analysis and preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "zp9MeKEys_er"
      },
      "cell_type": "markdown",
      "source": [
        "Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "MklTIulns_et"
      },
      "cell_type": "code",
      "source": [
        "data_dir = '../input'\n",
        "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'dermatofibroma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5TJH0EJs_eu"
      },
      "cell_type": "markdown",
      "source": [
        "This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vjs83mEls_ev"
      },
      "cell_type": "code",
      "source": [
        "def compute_img_mean_std(image_paths):\n",
        "    \"\"\"\n",
        "        computing the mean and std of three channel on the whole dataset,\n",
        "        first we should normalize the image from 0-255 to 0-1\n",
        "    \"\"\"\n",
        "\n",
        "    img_h, img_w = 224, 224\n",
        "    imgs = []\n",
        "    means, stdevs = [], []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        img = cv2.imread(image_paths[i])\n",
        "        img = cv2.resize(img, (img_h, img_w))\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.stack(imgs, axis=3)\n",
        "    print(imgs.shape)\n",
        "\n",
        "    imgs = imgs.astype(np.float32) / 255.\n",
        "\n",
        "    for i in range(3):\n",
        "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
        "        means.append(np.mean(pixels))\n",
        "        stdevs.append(np.std(pixels))\n",
        "\n",
        "    means.reverse()  # BGR --> RGB\n",
        "    stdevs.reverse()\n",
        "\n",
        "    print(\"normMean = {}\".format(means))\n",
        "    print(\"normStd = {}\".format(stdevs))\n",
        "    return means,stdevs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YV85Yymrs_ew"
      },
      "cell_type": "markdown",
      "source": [
        "Return the mean and std of RGB channels"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4RtaBC8rs_ex"
      },
      "cell_type": "code",
      "source": [
        "norm_mean,norm_std = compute_img_mean_std(all_image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHHGrINAs_ex"
      },
      "cell_type": "markdown",
      "source": [
        "Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0tugCQgXs_ey"
      },
      "cell_type": "code",
      "source": [
        "df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n",
        "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
        "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
        "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
        "df_original.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OnSB6Hmxs_ey"
      },
      "cell_type": "code",
      "source": [
        "# this will tell us how many images are associated with each lesion_id\n",
        "df_undup = df_original.groupby('lesion_id').count()\n",
        "# now we filter out lesion_id's that have only one image associated with it\n",
        "df_undup = df_undup[df_undup['image_id'] == 1]\n",
        "df_undup.reset_index(inplace=True)\n",
        "df_undup.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yvIms1Zws_ez"
      },
      "cell_type": "code",
      "source": [
        "# here we identify lesion_id's that have duplicate images and those that have only one image.\n",
        "def get_duplicates(x):\n",
        "    unique_list = list(df_undup['lesion_id'])\n",
        "    if x in unique_list:\n",
        "        return 'unduplicated'\n",
        "    else:\n",
        "        return 'duplicated'\n",
        "\n",
        "# create a new colum that is a copy of the lesion_id column\n",
        "df_original['duplicates'] = df_original['lesion_id']\n",
        "# apply the function to this new column\n",
        "df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
        "df_original.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NQvrryHSs_ez"
      },
      "cell_type": "code",
      "source": [
        "df_original['duplicates'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uiQikCMNs_e0"
      },
      "cell_type": "code",
      "source": [
        "# now we filter out images that don't have duplicates\n",
        "df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
        "df_undup.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qd9ZKt_Ns_e1"
      },
      "cell_type": "code",
      "source": [
        "# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n",
        "y = df_undup['cell_type_idx']\n",
        "_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n",
        "df_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "s76iJdgis_e1"
      },
      "cell_type": "code",
      "source": [
        "df_val['cell_type_idx'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vmaF0qovs_e1"
      },
      "cell_type": "code",
      "source": [
        "# This set will be df_original excluding all rows that are in the val set\n",
        "# This function identifies if an image is part of the train or val set.\n",
        "def get_val_rows(x):\n",
        "    # create a list of all the lesion_id's in the val set\n",
        "    val_list = list(df_val['image_id'])\n",
        "    if str(x) in val_list:\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'train'\n",
        "\n",
        "# identify train and val rows\n",
        "# create a new colum that is a copy of the image_id column\n",
        "df_original['train_or_val'] = df_original['image_id']\n",
        "# apply the function to this new column\n",
        "df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n",
        "# filter out train rows\n",
        "df_train = df_original[df_original['train_or_val'] == 'train']\n",
        "print(len(df_train))\n",
        "print(len(df_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sJUwx9hYs_e1"
      },
      "cell_type": "code",
      "source": [
        "df_train['cell_type_idx'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y_CXctO1s_e2"
      },
      "cell_type": "code",
      "source": [
        "df_val['cell_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuN6lle7s_e2"
      },
      "cell_type": "markdown",
      "source": [
        "**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QQZtUTy0s_e3"
      },
      "cell_type": "code",
      "source": [
        "# Copy fewer class to balance the number of 7 classes\n",
        "data_aug_rate = [15,10,5,50,0,40,5]\n",
        "for i in range(7):\n",
        "    if data_aug_rate[i]:\n",
        "        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
        "df_train['cell_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G__znfdgs_e3"
      },
      "cell_type": "markdown",
      "source": [
        "At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6xWeqrf0s_e3"
      },
      "cell_type": "code",
      "source": [
        "# # We can split the test set again in a validation set and a true test set:\n",
        "# df_val, df_test = train_test_split(df_val, test_size=0.5)\n",
        "df_train = df_train.reset_index()\n",
        "df_val = df_val.reset_index()\n",
        "# df_test = df_test.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tJj7g5bts_e3"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2. Model building"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZVQSE-6Ts_e4"
      },
      "cell_type": "code",
      "source": [
        "# feature_extract is a boolean that defines if we are finetuning or feature extracting.\n",
        "# If feature_extract = False, the model is finetuned and all model parameters are updated.\n",
        "# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Wm1cV8sys_e4"
      },
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18, resnet34, resnet50, resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet121\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    return model_ft, input_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4NbjfSos_e5"
      },
      "cell_type": "markdown",
      "source": [
        "You can change your backbone network, here are 4 different networks, each network also has sevaral versions. Considering the limited training data, we used the ImageNet pre-training model for fine-tuning. This can speed up the convergence of the model and improve the accuracy.\n",
        "\n",
        "There is one thing you need to pay attention to, the input size of Inception is different from the others (299x299), you need to change the setting of compute_img_mean_std() function"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HAH30vDvs_e5"
      },
      "cell_type": "code",
      "source": [
        "# resnet,vgg,densenet,inception\n",
        "model_name = 'densenet'\n",
        "num_classes = 7\n",
        "feature_extract = False\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "# Define the device:\n",
        "device = torch.device('cuda:0')\n",
        "# Put the model on the device:\n",
        "model = model_ft.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "INhNlojRs_e6"
      },
      "cell_type": "code",
      "source": [
        "# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
        "# norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
        "# define the transformation of the train images.\n",
        "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n",
        "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
        "                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
        "# define the transformation of the val images.\n",
        "val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize(norm_mean, norm_std)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pkpZMybRs_e7"
      },
      "cell_type": "code",
      "source": [
        "# Define a pytorch dataloader for this dataset\n",
        "class HAM10000(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = Image.open(self.df['path'][index])\n",
        "        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ar3QthDus_e7"
      },
      "cell_type": "code",
      "source": [
        "# Define the training set using the table train_df and using our defined transitions (train_transform)\n",
        "training_set = HAM10000(df_train, transform=train_transform)\n",
        "train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n",
        "# Same for the validation set:\n",
        "validation_set = HAM10000(df_val, transform=train_transform)\n",
        "val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "k610FC0-s_e7"
      },
      "cell_type": "code",
      "source": [
        "# we use Adam optimizer, use cross entropy loss as our loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jOSvLeXs_e7"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3. Model training"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2isYSGHXs_e8"
      },
      "cell_type": "code",
      "source": [
        "# this function is used during training process, to calculation the loss and accuracy\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "6sm-rGj5s_e8"
      },
      "cell_type": "code",
      "source": [
        "total_loss_train, total_acc_train = [],[]\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = AverageMeter()\n",
        "    train_acc = AverageMeter()\n",
        "    curr_iter = (epoch - 1) * len(train_loader)\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, labels = data\n",
        "        N = images.size(0)\n",
        "        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n",
        "        images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "        train_loss.update(loss.item())\n",
        "        curr_iter += 1\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
        "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
        "            total_loss_train.append(train_loss.avg)\n",
        "            total_acc_train.append(train_acc.avg)\n",
        "    return train_loss.avg, train_acc.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sEGEdlJTs_e8"
      },
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion, optimizer, epoch):\n",
        "    model.eval()\n",
        "    val_loss = AverageMeter()\n",
        "    val_acc = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            images, labels = data\n",
        "            N = images.size(0)\n",
        "            images = Variable(images).to(device)\n",
        "            labels = Variable(labels).to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            prediction = outputs.max(1, keepdim=True)[1]\n",
        "\n",
        "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n",
        "\n",
        "            val_loss.update(criterion(outputs, labels).item())\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
        "    print('------------------------------------------------------------')\n",
        "    return val_loss.avg, val_acc.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M_iqFu7ps_fG"
      },
      "cell_type": "code",
      "source": [
        "epoch_num = 10\n",
        "best_val_acc = 0\n",
        "total_loss_val, total_acc_val = [],[]\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n",
        "    total_loss_val.append(loss_val)\n",
        "    total_acc_val.append(acc_val)\n",
        "    if acc_val > best_val_acc:\n",
        "        best_val_acc = acc_val\n",
        "        print('*****************************************************')\n",
        "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n",
        "        print('*****************************************************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97pAr1dNs_fH"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4. Model evaluation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8J9orlVgs_fH"
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(num = 2)\n",
        "fig1 = fig.add_subplot(2,1,1)\n",
        "fig2 = fig.add_subplot(2,1,2)\n",
        "fig1.plot(total_loss_train, label = 'training loss')\n",
        "fig1.plot(total_acc_train, label = 'training accuracy')\n",
        "fig2.plot(total_loss_val, label = 'validation loss')\n",
        "fig2.plot(total_acc_val, label = 'validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "89k9dYvAs_fI"
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uM2bH7JLs_fI"
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_label = []\n",
        "y_predict = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader):\n",
        "        images, labels = data\n",
        "        N = images.size(0)\n",
        "        images = Variable(images).to(device)\n",
        "        outputs = model(images)\n",
        "        prediction = outputs.max(1, keepdim=True)[1]\n",
        "        y_label.extend(labels.cpu().numpy())\n",
        "        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n",
        "\n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_label, y_predict)\n",
        "# plot the confusion matrix\n",
        "plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n",
        "plot_confusion_matrix(confusion_mtx, plot_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zZ7JMYcjs_fI"
      },
      "cell_type": "code",
      "source": [
        "# Generate a classification report\n",
        "report = classification_report(y_label, y_predict, target_names=plot_labels)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Osw7lNXBs_fJ"
      },
      "cell_type": "code",
      "source": [
        "label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n",
        "plt.bar(np.arange(7),label_frac_error)\n",
        "plt.xlabel('True Label')\n",
        "plt.ylabel('Fraction classified incorrectly')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Skin lesion classification(acc>90%)(Pytorch)",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}